{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. This is important because it can improve the performance of a machine learning model, reduce overfitting, and decrease the computational cost. Feature selection helps in focusing on the most relevant data, thereby enhancing the model's accuracy and efficiency.\n",
    "\n",
    "Types of Feature Selection:\n",
    "1)Filter Methods:\n",
    "\n",
    "Variance Threshold: Removes features with low variance.\n",
    "Correlation Matrix with Heatmap: Identifies highly correlated features and removes the least significant ones.\n",
    "Statistical Tests: Uses statistical methods (e.g., Chi-Square, ANOVA) to select features based on their relationships with the target variable.\n",
    "\n",
    "2)Wrapper Methods:\n",
    "\n",
    "Recursive Feature Elimination (RFE): Recursively removes the least important features based on model performance.\n",
    "Forward Selection: Starts with no features and adds one feature at a time that improves the model the most.\n",
    "Backward Elimination: Starts with all features and removes the least significant features one at a time.\n",
    "\n",
    "3)Embedded Methods:\n",
    "\n",
    "Lasso (L1 Regularization): Adds a penalty equal to the absolute value of the magnitude of coefficients, effectively shrinking some coefficients to zero.\n",
    "Ridge (L2 Regularization): Adds a penalty equal to the square of the magnitude of coefficients but does not shrink them to zero.\n",
    "Tree-Based Methods: Uses decision tree algorithms (e.g., Random Forest, Gradient Boosting) to measure feature importance.\n",
    "Each method has its own advantages and is suited for different scenarios depending on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter methods:\n",
    "\n",
    "i) variance threshold :Variance Threshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "df=pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "df['target']=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=VarianceThreshold(0.3)\n",
    "X_reduced=selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Correlation Matrix with Heatmap: Identifies highly correlated features and removes the least significant ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "           Feature1  Feature2  Feature3  Feature4\n",
      "Feature1       NaN       NaN       NaN       NaN\n",
      "Feature2       NaN  1.000000 -0.408248       NaN\n",
      "Feature3       NaN -0.408248  1.000000       NaN\n",
      "Feature4       NaN       NaN       NaN       NaN\n",
      "Original DataFrame:\n",
      "    Feature1  Feature2  Feature3  Feature4\n",
      "0         0         2         0         3\n",
      "1         0         1         4         3\n",
      "2         0         1         1         3\n",
      "3         0         1         0         3\n",
      "4         0         1         1         3\n",
      "Reduced DataFrame:\n",
      "    Feature1  Feature4\n",
      "0         0         3\n",
      "1         0         3\n",
      "2         0         3\n",
      "3         0         3\n",
      "4         0         3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: 5 samples, 4 features\n",
    "data = {\n",
    "    'Feature1': [0, 0, 0, 0, 0],\n",
    "    'Feature2': [2, 1, 1, 1, 1],\n",
    "    'Feature3': [0, 4, 1, 0, 1],\n",
    "    'Feature4': [3, 3, 3, 3, 3]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\\n\", corr_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Set a threshold for high correlation\n",
    "threshold = 0.9\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Find columns with correlation above the threshold\n",
    "to_drop = [column for column in corr_matrix.columns if any(corr_matrix[column] > threshold)]\n",
    "\n",
    "# Drop highly correlated features\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "print(\"Reduced DataFrame:\\n\", df_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii)ANOVA (Analysis of Variance):\n",
    "Used for categorical target variables.\n",
    "Measures the difference between groups and the variance within groups.\n",
    "Example: In a classification problem, ANOVA can be used to determine if there are significant differences between the means of different classes for a feature.\n",
    "iv)Mutual Information:\n",
    "Measures the mutual dependence between two variables.\n",
    "Can be used for both classification and regression problems.\n",
    "Example: Higher mutual information indicates a stronger dependency between the feature and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA Selected features:\n",
      " [[2 0]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "ANOVA Scores:\n",
      " [       nan 0.60000175 0.73846143        nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\vs code\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [0 3] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "d:\\vs code\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Selected features:\n",
      " [[0 3]\n",
      " [4 3]\n",
      " [1 3]\n",
      " [0 3]\n",
      " [1 3]]\n",
      "Mutual Information Scores:\n",
      " [0.         0.         0.         0.13333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Sample data: 5 samples, 4 features\n",
    "X = np.array([\n",
    "    [0, 2, 0, 3],\n",
    "    [0, 1, 4, 3],\n",
    "    [0, 1, 1, 3],\n",
    "    [0, 1, 0, 3],\n",
    "    [0, 1, 1, 3]\n",
    "])\n",
    "y = np.array([0, 1, 0, 1, 0])  # Target variable (binary classification)\n",
    "\n",
    "# ANOVA (f_classif)\n",
    "anova_selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_anova_selected = anova_selector.fit_transform(X, y)\n",
    "\n",
    "print(\"ANOVA Selected features:\\n\", X_anova_selected)\n",
    "print(\"ANOVA Scores:\\n\", anova_selector.scores_)\n",
    "\n",
    "# Mutual Information\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=2)\n",
    "X_mi_selected = mi_selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Mutual Information Selected features:\\n\", X_mi_selected)\n",
    "print(\"Mutual Information Scores:\\n\", mi_selector.scores_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v)Chi-Squres : The Chi-Square test is a statistical test used to determine if there is a significant association between two categorical variables. In the context of feature selection, it is often used to select features that have the strongest relationship with the target variable in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features:\n",
      "    Feature1  Feature2  Feature3  Feature4\n",
      "0         0         2         1         3\n",
      "1         1         1         1         3\n",
      "2         0         2         0         3\n",
      "3         1         1         0         3\n",
      "4         0         2         1         3\n",
      "Selected features:\n",
      " [[0 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [0 2]]\n",
      "Chi-Square Scores:\n",
      " [3.         0.75       0.05555556 0.        ]\n",
      "Selected feature indices: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Sample data: 5 samples, 4 categorical features\n",
    "data = {\n",
    "    'Feature1': [0, 1, 0, 1, 0],\n",
    "    'Feature2': [2, 1, 2, 1, 2],\n",
    "    'Feature3': [1, 1, 0, 0, 1],\n",
    "    'Feature4': [3, 3, 3, 3, 3]\n",
    "}\n",
    "X = pd.DataFrame(data)\n",
    "y = np.array([0, 1, 0, 1, 0])  # Target variable (binary classification)\n",
    "\n",
    "# Initialize the Chi-Square feature selector\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "# Display the selected features\n",
    "print(\"Original features:\\n\", X)\n",
    "print(\"Selected features:\\n\", X_kbest)\n",
    "print(\"Chi-Square Scores:\\n\", chi2_selector.scores_)\n",
    "print(\"Selected feature indices:\", chi2_selector.get_support(indices=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
